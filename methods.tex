\section{Models and Experiments}

We run our analyis on models from three family series, Llama \citep{dubey2024llama}, Gemma \citep{team2024gemma}, and Mistral \citep{jiang2023mistral, jiang2024mixtral}. We use Meta-Llama 3.1 \{8, 70, 405\}B from the Llama series, Gemma 2 \{2, 9, 27\}B from the Gemma series, and Mistral 7B / Mixtral 8x\{7, 22\}B from the Mistral series. We limit our analysis to pre-trained base models and leave the analysis on post-trained / instruct models to future work. The model sizes range from 2B to 405B in our analysis.

We do a comprehensive analysis on five NLI benchmarks: ANLI \citep{nie-etal-2020-adversarial}, HansNLI \citep{mccoy-etal-2019-right}, MNLI \citep{williams-etal-2018-broad}, SNLI \citep{bowman-etal-2015-large}, and AbductiveNLI \citep{bhagavatula2020abductive}. The number of examples and labels for each dataset are present in Table \ref{tab:dataset}. We filter out the noisy samples from MNLI and SNLI where there's no ground truth available for the evaluation.

\begin{table}
  \centering
  \begin{tabular}{c|c|c}
    Benchmark & \# Samples & \# labels \\
    \hline
    ANLI & 1200 & 3 \\
    HansNLI & 30000 & 2 \\
    MNLI & 9815 & 3 \\
    SNLI & 9842 & 3 \\
    AbductiveNLI & 1532 & 2 \\
  \end{tabular}
\caption{Dataset details}
\label{tab:dataset}
\end{table}
