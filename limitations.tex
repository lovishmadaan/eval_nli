\section{Limitations}

One of the limitation of our work is that all our analysis is limited to pre-trained models. Post-training with supervised fine-tuning and RLHF might change the behaviour of models on NLI benchmarks. But, we do not believe this to be a major limitation as \citet{dubey2024llama} show that it's possible to improve the performance of models during the post-training phase, rather than lose it because of instruction fine-tuning and safety. Therefore, we believe that NLI benchmarks would be informative for post-trained models as well.
Another limitation is that we only consider the choice-based setup for evaluation. Using a generative setup might change model behaviour and correspondingly, the signal provided by NLI benchmarks. We leave this to future work.