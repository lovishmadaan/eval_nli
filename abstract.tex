\begin{abstract}
In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider models' ability to do natural language inference (NLI) tasks. 
In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, could still be informative for training or evaluating LLMs.
Focussing on five different NLI benchmarks across six different models, we investigate if they are able to discriminate models of different size and quality and how their accuracies develop during training.
Furthermore, we investigate the extent to which the softmax distributions of models align with human distributions in cases where questions are ambiguous or vague.
Overall, our results paint a positive picture for the NLI tasks: we find that they are able to discriminate well between models at various stages of training, yet are not (all) saturated.
Furthermore, we find that while the distributional similarity with human label distributions improves with scale, it is still far above distributional similarity between two populations of humans, making it a potentially interesting statistic to consider.
\end{abstract}
