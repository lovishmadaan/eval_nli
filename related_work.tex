\section{Related work}

Before presenting our analysis, we %first 
discuss NLI tasks (\cref{related:nli}) and touch upon the related topic of subjectivity in NLP tasks (\cref{related:subjectivity}).

\subsection{RTE tasks and their results}
\label{related:nli}

In RTE tasks -- also referred to as `natural language inference' (NLI) tasks, models are asked to judge whether the meaning of one sentence can be inferred from the meaning of another.
Because the concept of entailment (and contradiction) are considered central to many aspects of language meaning \citep[e.g.][]{bowman-etal-2015-large}, such tasks were for some period considered an important task to determine whether one model could understand language better than another \citep{poliak-2020-survey}.
% For a long time, Recognizing Textual Entailment (RTE) tasks .\footnote{https://aclweb.org/aclwiki/index.php?\\title=Textual\_Entailment\_Resource\_Pool} were used as testbeds for NLI. 
% \dieuwke{Explain what a textual entailment task and natural language inference is is and why it was considered a good way to evaluate NLP models.}
Over the years, many different, increasingly more difficult NLI tasks have been proposed in the literature.
Included in the popular benchmarks GLUE \citep{wang2019glue} and SuperGLUE \citep{wang2019superglue}, the benchmarks MNLI \citep{williams-etal-2018-broad} and RTE \citep{dagan2005pascal} and RTE were used to claim SOTA by many influential models \citep[e.g.][]{raffel2023t5,devlin2019BERT}.
When performance on MNLI and RTE started to saturate, several adversarial NLI benchmarks were introduced, among which ANLI \citep{raffel2023t5} and HANS \citep{mccoy-etal-2019-right}, on which BERT-style models performed poorly compared to their previous versions.
For LLMs, NLI benchmarks are rarely used.
Of all big LLM releases, only GPT-3 \citep{brown2020language} reported an NLI score, and only on one partition of ANLI.
They concluded that NLI is a difficult task for general purpose LLMs.
Similar trends were observed by \citet{ohmer2024form} and \citet{weber-etal-2023-mind} for decoder-only LLMs on various NLI tasks, in part motivating the study presented here.
% 
% With the exception of \citet{brown2020language},  who reported low scores for GPT-3 on the NLI benchmark \emph{Adversarial NLI} \citep[ANLI,][]{nie-etal-2020-adversarial}, not a single LLM release paper considers an NLI benchmark in their evaluation suite.\footnote{However, some recent papers have reported low scores for GPT3.5 for XNLI \citep{ohmer2024form,ohmer-etal-2023-separating} and for Llama 1 models on ANLI, HANS \citep{mccoy-etal-2019-right} and MNLI \citep{weber-etal-2023-mind}.}
% 
% 
% 
% % 
% % % \citet{bhagavatula2020abductive} released $\alpha$NLI to test abductive reasoning in models. Instead of a single hypothesis as in standard NLI tasks, $\alpha$NLI presents an observation and a pair of hypotheses to the model, and the task is to choose the most plausible hypothesis.
% % ANLI \citep{nie-etal-2020-adversarial} used an iterative, adversarial human-and-model-in-the-loop method to come up with an adversarial version of NLI that is challenging for the models existing then.
% % Similarly, HANS \citep{mccoy-etal-2019-right} is another adversarial dataset where models that perform MNLI did not necessary perform better on HANS, because these models have adopted certain fallible heuristics during their training.
% 
% \subsection{NLI scores across time}
% \label{related:scores}
% % \textcolor{red}{Dieuwke: I left some notes in this subsection, but my overall suggestion would be to focus a bit more on how well models were doing and make it less opinionated. E.g. one could say that models were initally not performing very well but the scores quickly saturated, upon which new datasets were brought out that were more challenging.
% % Also give the current state of difficulty, which would include the results with LLMs from the section below (I would put it here and remove it from the next section.}
% 
% Popular benchmarks like GLUE \citep{wang2019glue} and SuperGLUE \citep{wang2019superglue} also contain entailment benchmarks like MNLI \citep{williams-etal-2018-broad} and RTE \citep{dagan2005pascal}.
% Specifically, the MNLI task in GLUE was a reliable benchmark because models that were better at MNLI were more often than not better elsewhere \citep{raffel2023t5,zaheer2020big}.
% A lot of the models and systems based on BERT \citep{devlin2019bert} were proposed that tried to reach SoTA on GLUE, SuperGLUE, and NLI tasks, and match human performance on these benchmarks \citep{he2021deberta,he2021debertav3,patra2022englishcentric}. 
% The pre-training and fine-tuning regime quickly started saturating these benchmarks \citep{raffel2023t5}. Specifically for NLI, adversarial and harder datasets like ANLI \citep{nie-etal-2020-adversarial} and HANS \citep{mccoy-etal-2019-right} were introduced where BERT-like models did not perform as well compared to tasks like MNLI and SNLI.
% \citet{brown2020language} introduced GPT-3 and the era of benchmarking with zero shot or few shot examples. 
% The idea is when training general purpose language models, you do not need task specific fine-tuning. 
% GPT-3's performance on RTE and ANLI was near chance accuracy and lower than fine-tuned BERT and RoBERTa models \citep{liu2019roberta}, thus concluding that NLI is a very difficult task for general purpose large language models.


\subsection{Subjectivity in NLP tasks}
\label{related:subjectivity}
Another line of work relevant to ours considers behaviour of models in cases where humans disagree on what is the correct label for a particular sample. 

The ground truth labels for benchmarks like MNLI and SNLI are decided according to the majority label by human annotators. 
This simplifies the data annotation process while also making the evaluation easier by framing it as a classification task. 
However, \citet{nie-etal-2020-learn} showed that significant disagreement exists in a large amount of these datasets because the meaning of a sentence can differ based on the context and background knowledge, and the ground truth label according to a human annotator depends on their understanding of this context. 
They release ChaosNLI, a dataset comprising 100 annotations for each sample for a subset of three benchmarks - MNLI, SNLI, and $\alpha$NLI to better understand the distributional properties and entropy of labels for these benchmarks.

Most important to our study, \citet{chen2024seeingbig} explore whether the softmax probability distributions of 2 LLMs across the labels in ChaosNLI \citep{nie-etal-2020-learn} and VariErr NLI \citep{weber-genzel-etal-2024-varierr} can approximate the Human Judgement Distribution (HJD) across the labels, thus providing a scalable way for annotations
.
