\section{Related work}

Before presenting our analysis, we first discuss NLI or \emph{recognising textual entailment} (RTE) tasks (\cref{related:nli}) and touch upon the related topic of subjectivity for NLP tasks (\cref{related:subjectivity}) before presenting our analysis.

\subsection{RTE tasks and their results}
\label{related:nli}

In RTE tasks -- also referred to as `natural language inference' (NLI) tasks, models are asked to judge whether the meaning of one sentence can be inferred from the meaning of another.
Because the concept of entailment (and contradiction) are considered central to many aspects of language meaning \citep[e.g.][]{bowman-etal-2015-large}, such tasks were for some period considered an important task to determine whether one model could understand language better than another \citep{poliak-2020-survey}.
Over the years, many different, increasingly more difficult NLI tasks have been proposed in the literature.
Included in the popular benchmarks GLUE \citep{wang2019glue} and SuperGLUE \citep{wang2019superglue}, the benchmarks MNLI \citep{williams-etal-2018-broad} and RTE \citep{dagan2005pascal} and RTE were used to claim SoTA by many influential models such as BERT \citep{devlin2019bert} and T5 \citep{raffel2023t5}.
When performance on MNLI and RTE started to saturate, several adversarial NLI benchmarks were introduced, such as ANLI \citep{raffel2023t5} and HANS \citep{mccoy-etal-2019-right}, on which BERT-style models performed poorly compared previous datasets.
For LLMs, NLI benchmarks are rarely used.
Of all big LLM releases, only GPT-3 \citep{brown2020language} reported an NLI score, and only on one partition of ANLI.
They concluded that NLI is a difficult task for general purpose LLMs.
Similar trends were observed by \citet{ohmer2024form} and \citet{weber-etal-2023-mind} for decoder-only LLMs on various NLI tasks, in part motivating the study presented here.

\subsection{Subjectivity in NLP tasks}
\label{related:subjectivity}
Another line of work relevant to ours considers the behaviour of models in cases where humans disagree on the correct label for a particular sample \citep[for an overview, see][]{plank-2022-problem}.
The ground truth labels for NLP benchmarks are often decided according to the majority label by human annotators. 
This simplifies the data annotation process, while also making the evaluation easier. 
However, several previous studies have noted that human disagreements in annotations for NLP datasets reflect the lack of a single ground truth label, rather than than noise in the annotation process \citep[e.g.][]{de-marneffe-etal-2012-happen, plank-etal-2014-learning,pavlick-kwiatkowski-2019-inherent,nie-etal-2020-learn}and that we could benefit from embracing rather than resolving it \citep{plank-2022-problem,aroyo2015truthIA}.

For NLI, label disagreements were captured in more detail in the dataset ChaosNLI \citep{nie-etal-2020-learn}, comprising of 100 annotations for each sample for a subset of three benchmarks -- MNLI, SNLI, and $\alpha$NLI.
As human disagreements are ubiquitous yet mostly ignored in LLM evaluation, in our study, we analyse not only the model accuracies, but also the relationship between these disagreements and models' probability distributions across the different labels.
A study with a similar aim is presented by \citet{chen2024seeingbig}. They explore whether the softmax probability distributions solicited by a few explanations from two LLMs (Mixtral and Llama 3) can approximate human judgement distributions (HJD) on the ChaosNLI and VariErr NLI \citep{weber-genzel-etal-2024-varierr} benchmarks.
Similarly, \citet{baan-etal-2024-interpreting} and \citet{lee-etal-2023-large} compare human and model judgement distributions on ChaosNLI, finding that models fail to capture human distributions in LLMs and model confidences. 
Furthermore, \citet{baan-etal-2024-interpreting} argue how the softmax probability distribution can be interpreted as both an approximation to human label distribution and confidence estimation in language models.
