\section{Related work}

As mentioned before, NLI tasks used to be a popular way to evaluate NLP models.
In this section, we discuss common NLI benchmarks (\cref{related:benchmarks}) and their scores for masked language models and MLMs (\cref{related:mlms} and \cref{related:llms}, respectively).
Further relevant to our work, we also discuss work related to research into the behaviour of NLP models on subjective tasks (\cref{related:subjective}).

\subsection{Natural Language Inference as a means of evaluation}
\label{related:benchmarks}

% For a long time, Recognizing Textual Entailment (RTE) tasks\footnote{https://aclweb.org/aclwiki/index.php?\\title=Textual\_Entailment\_Resource\_Pool}  were used as testbeds for NLI. These are hand-labeled and usually the sample size is very small.
% 
% % \citet{bhagavatula2020abductive} released $\alpha$NLI to test abductive reasoning in models. Instead of a single hypothesis as in standard NLI tasks, $\alpha$NLI presents an observation and a pair of hypotheses to the model, and the task is to choose the most plausible hypothesis.
% ANLI \citep{nie-etal-2020-adversarial} used an iterative, adversarial human-and-model-in-the-loop method to come up with an adversarial version of NLI that is challenging for the models existing then.
% Similarly, HANS \citep{mccoy-etal-2019-right} is another adversarial dataset where models that perform MNLI did not necessary perform better on HANS, because these models have adopted certain fallible heuristics during their training.

\subsection{BERT and friends}
\label{related:mlms}
The popularity of GLUE \citep{wang2019glue} and SuperGLUE \citep{wang2019superglue} arose from the fact that there was significant visible gap between human and model performance at that time. Specifically, the MNLI task in GLUE was a reliable benchmark because of the monotonicity during fine-tuning of models and models that were better at MNLI were more often than not better elsewhere. A lot of the models and systems based on BERT \citep{devlin2019bert} were proposed that tried to reach SoTA on GLUE/SuperGLUE and match human performance on these benchmarks \citep{he2021deberta,he2021debertav3,patra2022englishcentric}. The pre-training and fine-tuning regime quickly started saturating these benchmarks.

\subsection{The era of LLMs}
\label{related:llms}
\citet{brown2020language} introduced GPT-3 and the era of benchmarking with zero shot or few shot examples. The idea is when training general purpose language models, you do not need task specific fine-tuning. GPT-3's performance on RTE and ANLI was near chance accuracy, thus concluding that NLI is a very difficult task for general purpose large language models. Similar trends were observed by \citet{ohmer2024form} and \citet{weber-etal-2023-mind} for decoder-only LLMs on various NLI tasks. \citet{chen2024seeingbig} explore whether the softmax probability distributions of 2 LLMs across the labels in ChaosNLI \citep{nie-etal-2020-learn} and VariErr NLI \citep{weber-genzel-etal-2024-varierr} can approximate the Human Judgement Distribution (HJD) across the labels, thus providing a scalable way for annotations.

\subsection{Subjective NLP tasks}
\label{related:subjective}

% \begin{itemize}
%   \item Benchmarks
%   \item Fine-tuned performance (BERT/T5/etc etc)
%   \item LLM performance (GPT-3 / Dieukwe's papers)
% \end{itemize}
