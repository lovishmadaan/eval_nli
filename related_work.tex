\section{Related work}

\dieuwke{Redo introduction of this section}
% As mentioned before, NLI tasks used to be a popular way to evaluate NLP models.
% In this section, we discuss common NLI benchmarks (\cref{related:benchmarks}) and their scores for masked language models and MLMs (\cref{related:mlms} and \cref{related:llms}, respectively).
% Further relevant to our work, we also discuss work related to research into the behaviour of NLP models on subjective tasks (\cref{related:subjective}).

\subsection{Natural Language Inference as a means of evaluation}
\label{related:benchmarks}

% For a long time, Recognizing Textual Entailment (RTE) tasks .\footnote{https://aclweb.org/aclwiki/index.php?\\title=Textual\_Entailment\_Resource\_Pool} were used as testbeds for NLI. 
% \dieuwke{Explain what a textual entailment task and natural language inference is is and why it was considered a good way to evaluate NLP models.}

Understanding textual entailment is fundamental to language and semantic understanding. Presenting two sentences and the ability to infer entailment or contradiction highlights a model's ability to do commonsense reasoning. To this end, various benchmarks were introduced to quantify this commonsense reasoning ability like RTE \citep{dagan2005pascal}, MNLI \citep{williams-etal-2018-broad}, and SNLI \citep{bowman-etal-2015-large}.

% 
% % \citet{bhagavatula2020abductive} released $\alpha$NLI to test abductive reasoning in models. Instead of a single hypothesis as in standard NLI tasks, $\alpha$NLI presents an observation and a pair of hypotheses to the model, and the task is to choose the most plausible hypothesis.
% ANLI \citep{nie-etal-2020-adversarial} used an iterative, adversarial human-and-model-in-the-loop method to come up with an adversarial version of NLI that is challenging for the models existing then.
% Similarly, HANS \citep{mccoy-etal-2019-right} is another adversarial dataset where models that perform MNLI did not necessary perform better on HANS, because these models have adopted certain fallible heuristics during their training.

\subsection{NLI scores across time}
\label{related:mlms}
% \textcolor{red}{Dieuwke: I left some notes in this subsection, but my overall suggestion would be to focus a bit more on how well models were doing and make it less opinionated. E.g. one could say that models were initally not performing very well but the scores quickly saturated, upon which new datasets were brought out that were more challenging.
% Also give the current state of difficulty, which would include the results with LLMs from the section below (I would put it here and remove it from the next section.}

Popular benchmarks like GLUE \citep{wang2019glue} and SuperGLUE \citep{wang2019superglue} also contain entailment benchmarks like MNLI \citep{williams-etal-2018-broad} and RTE \citep{dagan2005pascal}.
Specifically, the MNLI task in GLUE was a reliable benchmark because models that were better at MNLI were more often than not better elsewhere \citep{raffel2023t5,zaheer2020big}.
A lot of the models and systems based on BERT \citep{devlin2019bert} were proposed that tried to reach SoTA on GLUE, SuperGLUE, and NLI tasks, and match human performance on these benchmarks \citep{he2021deberta,he2021debertav3,patra2022englishcentric}. 
The pre-training and fine-tuning regime quickly started saturating these benchmarks \citep{raffel2023t5}. Specifically for NLI, adversarial and harder datasets like ANLI \citep{nie-etal-2020-adversarial} and HANS \citep{mccoy-etal-2019-right} were introduced where BERT-like models did not perform as well compared to tasks like MNLI and SNLI.
\citet{brown2020language} introduced GPT-3 and the era of benchmarking with zero shot or few shot examples. 
The idea is when training general purpose language models, you do not need task specific fine-tuning. 
GPT-3's performance on RTE and ANLI was near chance accuracy and lower than fine-tuned BERT and RoBERTa models \citep{liu2019roberta}, thus concluding that NLI is a very difficult task for general purpose large language models.
Similar trends were observed by \citet{ohmer2024form} and \citet{weber-etal-2023-mind} for decoder-only LLMs on various NLI tasks. 


\subsection{Subjectivity in NLP tasks}
\label{related:subjectivity}
As we will see later, another line of work relevant to ours considers behaviour of models in cases where humans disagree on what is the correct label for a particular sample. The ground truth labels for benchmarks like MNLI and SNLI are decided according to the majority label by human annotators. This simplifies the data annotation process while also making the evaluation easier by framing it as a classification task. However, \citet{nie-etal-2020-learn} showed that significant disagreement exists in a large amount of these datasets because the meaning of a sentence can differ based on the context and background knowledge, and the ground truth label according to a human annotator depends on their understanding of this context. They release ChaosNLI, a dataset comprising 100 annotations for each sample for a subset of three benchmarks - MNLI, SNLI, and $\alpha$NLI to better understand the distributional properties and entropy of labels for these benchmarks.
Most important to our study, \citet{chen2024seeingbig} explore whether the softmax probability distributions of 2 LLMs across the labels in ChaosNLI \citep{nie-etal-2020-learn} and VariErr NLI \citep{weber-genzel-etal-2024-varierr} can approximate the Human Judgement Distribution (HJD) across the labels, thus providing a scalable way for annotations.
