\section{Results}

Now we present our results, focussing in particular on whether NLI benchmarks provide a discriminative signal for fully trained out models (\cref{subsec:fully_trained}), how their performance develops during training (\cref{subsec:during_training}), whether there is still room for improvement (\cref{subsec:saturation}), and how model judgements compare to human judgements in case of ambiguous or vague questions (\cref{subsec:chaosnli}).

% \subsection{Performance on fully trained models}
\subsection{Informativeness for fully trained models}\label{subsec:fully_trained}

\begin{figure*}[t]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/legend}
        \vspace{-2mm}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.217\textwidth}
    \centering
    \includegraphics[height=3.2cm]{figures/anli}
    \caption{ANLI}
    \end{subfigure}
    \label{fig:anli}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=25mm 0 0 0, clip]{figures/hansnli}
    \caption{HANS}
    \label{fig:hans}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=25mm 0 0 0, clip]{figures/mnli_matched}
    \caption{MNLI}
    \label{fig:mnli}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=25mm 0 0 0, clip]{figures/snli}
    \caption{SNLI}
    \label{fig:snli}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=25mm 0 0 0, clip]{figures/abductivenli}
    \caption{$\alpha$NLI}
    \label{fig:alphanli}
    \end{subfigure}
    \caption{\textbf{Performance across shots.} We show performance across shots for nine trained-out models, for ANLI, HANS, MNLI, SNLI and AbductiveNLI. Dashed lines indicate Random, and Finetuned-BERT baselines.}\label{fig:shot_performance}
\end{figure*}

% \paragraph{Q1: Does NLI provide signal for LLMs?}
% We show \begin{enumerate}
%     \item curves + scores on trained-out models.  Conclusion: ? Should say something about whether it is separating models in early stages of training, and whether it is monotonic, how much variance etc.
%     \item Trained out model scores. Tentative conclusion (pending numbers): with the exception of abductive NLI, these datasets allow to distiguish models of different sizes. Doesn't work at all with zero-shot (which can explain some previous results?), but with one or two shots we get decent scores. ANLI seems to be the most challenging with around 70\% accuracy on the 405B model.
%     \item Ablations: conclusion??
% \end{enumerate}

With the goal to get an overall estimate of the difficulty of the task and the extent to which it can discriminate models, we first consider the performance of fully pre-trained models from the respective model series.

\paragraph{Accuracy across shots}
For each of the models, we compute results with variable number of shots.
We report the results in \cref{fig:shot_performance}, along with a chance baseline and the results of a fine-tuned BERT model.
For all models, we observe rather poor zero-shot performance for all tasks, except $\alpha$NLI, confirming previously reported results by \citet{ohmer2024form} and \citet{weber-etal-2023-mind}, among others.
When more shots are added performance starkly improves.
Even with just one shot, the performance is significantly better than the zero shot accuracy.
Adding more than three or four examples in the few shots does not improve performance much and saturates around 10 shots. 
Among the five benchmarks, the most challenging benchmark is ANLI.
Although the larger models in the Llama, Gemma, and Mistral series far outperform the finetuned BERT baseline, they do not exceed 70\% accuracy -- to some extent confirming the difficulty of the benchmark reported by \citet{brown2020language}.

\paragraph{Model discriminability}
In terms of discriminability, virtually all benchmarks provide a clear gap between the smaller and larger models.
For example, for the Llama series, 405B performs the best followed by 70B and then the 8B.
Though these three models are trained on the same amount of text tokens, performance clearly improves with scale.
The exception to this pattern is $\alpha$NLI, which appears to be near-saturated already at 70B scale, at a performance of around 85\%.
We conclude, from these results, that the benchmarks under consideration provide a useful signal to compare trained-out models, though it is unclear to what extent their performance has saturated, a question that we discuss in more detail later in this paper (\cref{subsec:saturation}).
Gemma-2 9B model has chance accuracy and has similar performance to the 2B model and does not show this scaling behaviour, but it's visible with the 27B model scale. This is not the case in its counterparts in the Mistral and Llama series, where the smaller models perform significantly better and the larger models in the respective model families are even better.

% \subsection{Performance development over training}
\subsection{Informativeness during training}\label{subsec:during_training}

Next, we investigate whether NLI datasets may provide a good signal during training.
To this end, we pre-train Llama-3 architecture based 8B and 70B models from scratch for 2T tokens.
The details of the pre-training mix are provided in \cref{sec:appendix}. 

\paragraph{Development during training} In \cref{fig:performance_training}, we show how (four-shot) performance develops during training.
We see that for most benchmarks, the 8B and 70B model quickly start to diverge.
The 70B model starts improving after around 250B tokens, for ANLI and $\alpha$NLI, it crosses fine-tuned BERT performance after around 500B tokens.
On the other hand, the development of performance for the 8B model is slow, at this scale not exceeding chance accuracy for HansNLI, MNLI, and SNLI.
From the final model performance of 8B depicted in Figure \cref{fig:shot_performance}, we can conclude that the 8B model improves fairly late during the training stage.
We did not have the budget to do the full pre-training run, but longer training definitely seems to help on NLI tasks, further confirming the claim that NLI models can still provide a helpful signal.

\begin{figure*}[t]
    \begin{subfigure}[b]{\textwidth}
        %\centering
        \includegraphics[width=0.3\textwidth]{figures/training_legend}
        \vspace{-2mm}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.20\textwidth}
    \centering
    \includegraphics[height=3.2cm]{figures/anli_intermediate}
    \caption{ANLI}
    \end{subfigure}
    \label{fig:anli_int}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=11mm 0 0 0, clip]{figures/hansnli_intermediate}
    \caption{HANS}
    \label{fig:hans_int}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=11mm 0 0 0, clip]{figures/mnli_matched_intermediate}
    \caption{MNLI}
    \label{fig:mnli_int}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=11mm 0 0 0, clip]{figures/snli_intermediate}
    \caption{SNLI}
    \label{fig:snli_int}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
    \centering
    \includegraphics[height=3.2cm, trim=11mm 0 0 0, clip]{figures/abductivenli_intermediate}
    \caption{$\alpha$NLI}
    \label{fig:alphanli_int}
    \end{subfigure}
    \caption{\textbf{Performance during training.} We show how performance for the five benchmarks develops during training, for two Llama-3 style models.}\label{fig:performance_training}
\end{figure*}

\paragraph{Utility for ablations}
A requirement for a benchmark to provide a useful signal during training is that it develops relatively monotonically during training.
The plots in \cref{fig:performance_training} suggest that this is not the case for most of the benchmarks for the 8B model.
Following \citet{variancepaper}, we quantify the benchmarks' monotonicity on both discrete (accuracy) and continuous metrics (NLL).%
\footnote{Tracking continuous metrics like negative log likelihood (NLL) over the course of the training generally provides more monotonic results than discrete metrics such as accuracy.}
In \cref{tab:monotonicity}, we can see that, despite the benchmarks' moderate sizes, monotonicity is low for accuracy as well as NLL, suggesting that the benchmark may not be suitable for monitoring performance on small number of closely-spaced checkpoints for smaller models. Monotonicity for the 70B is relatively high, but the compute involved for doing ablations increases 10x for stable signals.

\begin{table}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lcccc}
    %\toprule
    %Model $\rightarrow$ 
    & \multicolumn{2}{c}{8B} & \multicolumn{2}{c}{70B} \\
    % \toprule
    %Benchmark $\downarrow$
    & $mon_{disc}$  & $mon_{cont}$ & $mon_{disc}$ & $mon_{cont}$ \\
    \toprule
    $\alpha$NLI & 0.62 & 0.62 & 0.79 & 0.79 \\
    \midrule
    ANLI & - & - & 0.67 & 0.47 \\
    \midrule
    HANS & 0.32 & 0.46 & 0.57 & 0.63 \\
    \midrule
    MNLI & 0.34 & 0.51 & 0.77 & 0.80 \\
    \midrule
    SNLI & 0.05 & 0.38 & 0.64 & 0.65 \\
    \bottomrule
    \end{tabular}
}
\caption{Monotonicity values for the 8B and 70B models during the course of the training. We report both the discrete ($mon_{disc}$) and continuous ($mon_{cont}$) monotonicity values.}
\label{tab:monotonicity}
\end{table}

\subsection{Dataset saturation}\label{subsec:saturation}
Having confirmed that the NLI benchmarks under scrutiny provide discrimination between LLMs of different sizes, we now turn to the question of saturation: the results show that the benchmarks would have been useful so far, but how about the future?
As already pointed out before, the benchmark that still has the clearest room for improvement is adversarial benchmark ANLI, with performances not exceeding 70\% even for the largest models.
For the other benchmarks performances are substantially higher, and it is unclear to what extent the benchmarks may suffer saturation.
To address this question, we first conduct a manual error analysis on the examples that the largest models assign an incorrect label.

We analyse the predictions of Llama 3.1 405B model on the MNLI subset of ChaosNLI under a 4-shot setup. We consider those samples where the model prediction deviates from the human annotator majority. We can broadly divide the errors made by the model into two categories i) where the model is actually wrong and ii) where the humans themselves disagree on the actual ground truth label. We show some representative examples in Table \ref{tab:sample_analysis}. In the first example, the majority label deviates from the original ground truth and the model prediction. In fact, the interpretation of this example is a bit ambiguous and the hypothesis can either entail or is neutral to the premise depending on the interpretation. Second example is similar as we don't know the tone of the speaker, if they are sympathetic, then the hypothesis contradicts the premise otherwise it entails the premise. The model predictions for the third and fourth examples are clearly wrong where the majority label also matches the original ground truth.



\begin{table*}[t]
\centering
\small
\begin{tabular}{p{8cm}|c|c|c}
\toprule
\textbf{Example} & \textbf{Label Distribution} & \textbf{Og.} & \textbf{Prediction} \\
\midrule
\textbf{Premise}: you want to punch the button and go\\
\textbf{Hypothesis}: You don't want to push the button lightly, but rather punch it hard. & \textbf{e: 48}, n: 45, c: 7 & n & n \\
\midrule
\textbf{Premise}: Sorry but that's how it is.\\
\textbf{Hypothesis}: This is how things are and there are no apologies about it. & \textbf{e: 48}, c: 40, n: 12 & c & c \\
\midrule
\textbf{Premise}: of course you could annex Cuba but they wouldn't like that a bit\\
\textbf{Hypothesis}: Annexing Cuba is a great idea. & e: 0, n: 31, \textbf{c: 69} & c & n \\
\midrule
\textbf{Premise}: Savonarola burned in Florence\\
\textbf{Hypothesis}: Florence became Savonarola's new home. & e: 3, n: 40, \textbf{c: 57} & c & n \\
\bottomrule
\end{tabular}
\caption{Sample Analysis}
\label{tab:sample_analysis}
\end{table*}

\dieuwke{Insert some manual analysis + possible conclusion: there seems to be some room for improvement, but oftentimes incorrect predictions are on samples where humans may disagree too.}
In the next section, we dive further into this phenomenon, investigating the relationship between human disagreements and model judgements.

% To substantiate this claim, we consider chaosNLI.
% We show 1) entropy plots for various models, these show that the larger models have pretty much perfect perfrmance on samples where humans agree across the board, but drop off on other examples. Smaller models don't get perfect performance also on the former. Results also show that a small amount of errors is due to annotation errors (or not matching majority labels), because the `corrected' accuracy is slightly higher than the non-corrected accuracy.
% 2) We show KL-divergence with human distributions. Conclusion: this gets better with scale (contradicts earlier findings!), but still far away from human alignment. Still room for improvement. We check how stable this signal is across training (so we show curves of KL/JSD and see whether this provides a astable signal.

\subsection{Model judgements in case of disagreement}\label{subsec:chaosnli}

% Benchmark: mnli_matched
% Number of examples: 1599
% Number of flipped labels: 508
% Number of same labels: 1091
% Percentage of flipped labels: 31.77
% 
% Benchmark: snli
% Number of examples: 1514
% Number of flipped labels: 378
% Number of same labels: 1136
% Percentage of flipped labels: 24.97
% 
% Benchmark: abductivenli
% Number of examples: 1532
% Number of flipped labels: 163
% Number of same labels: 1369
% Percentage of flipped labels: 10.64

To support our analysis, we utilise the dataset ChaosNLI \citep{nie-etal-2020-learn}, which contains 100 human annotations for over 1500 samples of the datasets MNLI, SNLI and $\alpha$NLI, each.
Using this data, we can i) estimate the extent to which suboptimal performances are due to cases where the dataset label does not match the majority label, and ii) investigate how model judgements change as human uncertainty on the task increases.

\paragraph{Majority accuracy}
First, we consider how models' accuracies changes when we replace the original labels with the majority label of the ChaosNLI dataset.
This changes 32\%, 25\% and 11\% of the labels of MNLI, SNLI and $\alpha$NLI, respectively.
In \cref{tab:chaos_acc}, we can see that, in some cases, the results differ per model and benchmark.
The largest effects are observed for MNLI, where for some models an increase of more than 10\%point is observed and the average accuracy across models is more than five points higher on the `corrected' datasets.
For the other two benchmarks, the results are more mixed, with little to no difference on average.
Only for the largest Llama3.1 model, the majority accuracy is systematically higher than the original accuracy, suggesting it may have honed in more on the majority label.
Interestingly, the MNLI and SNLI subsets of ChaosNLI appear substantially more difficult than the average dataset;
Even the Llama3.1 405B model stays below 70\% on both these subsets, suggesting that there is room for improvement on both these datasets.

\begin{figure*}
    \begin{subfigure}[b]{0.65\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{llcccccc}
        & & \multicolumn{2}{c}{\textbf{$\alpha$NLI}} & \multicolumn{2}{c}{\textbf{MNLI}} & \multicolumn{2}{c}{\textbf{SNLI}} \\
        \multicolumn{2}{c}{\textbf{Model}} & \textit{Og.} & \textit{Maj.} & \textit{Og.} & \textit{Maj.} & \textit{Og.} & \textit{Maj.}\\
        \toprule
        Gemma-2 & 2B & \textbf{55.55} & 54.31 & 33.27 & \textbf{42.59} & \textbf{34.94} & 30.78 \\
        Gemma-2 & 9B & \textbf{56.14} & 55.22 & 34.33 & \textbf{48.03} & \textbf{36.59} & 31.70 \\
        Gemma-2 & 27B & 81.98 & \textbf{84.01} & \textbf{54.72} & 52.78 & 64.73 & \textbf{65.19} \\
        \midrule
        Llama-3.1 & 8B & 77.55 & \textbf{78.00} & 49.47 & \textbf{50.97} & \textbf{55.28} & 55.48 \\
        Llama-3.1 & 70B & 86.36 & \textbf{87.21} & 57.66 & \textbf{67.54} & \textbf{60.44} & 58.52 \\
        Llama-3.1 & 405B & 85.51 & \textbf{86.10} & 64.04 & \textbf{69.67} & 64.60 & \textbf{67.31} \\
        \midrule
        Mistral & 7B & 74.41 & \textbf{75.78} & 49.97 & \textbf{53.22} & \textbf{49.47} & 48.15 \\
        Mixtral & 8x7B & \textbf{82.44} & 81.59 & \textbf{54.03} & 51.53 & 63.14 & \textbf{64.27} \\
        Mixtral & 8x22B & \textbf{84.14} & 83.68 & 60.23 & \textbf{67.04} & 64.86 & \textbf{67.83} \\
        \midrule
        \multicolumn{2}{c}{\emph{Average}} & 76.01 & 76.21 & 50.86 & 55.93 & 54.90 & 54.35 \\
    \bottomrule
    \end{tabular}}
    \caption{}
    \label{tab:chaos_acc}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/jsd_per_model}
        \caption{}
        \label{fig:jsd_per_model}
    \end{subfigure}
\caption{\textbf{Majority accuracy and JSD per model.} \dieuwke{update caption}}
\end{figure*}

In \cref{fig:entropy_accuracy}, we can furthermore see that, with the exception of the two smallest Gemma models, all models perform `better' on samples where the entropy of the labels is low.
For the larger models, this effect is larger: on samples where humans have high disagreement (and the majority label is thus in a way more representative of the average human judgements), their accuracies are often near maximal, and they drop down as human judgements become more dispersed.
 
\dieuwke{TODO: Add just two representative examples here and refer to appendix for all models.}

\subsection{Model versus human distributions}\label{subsec:chaosnli_dist}
The ChaosNLI dataset does not only help us estimate the adequacy of the original label sets of the respective datasets, it also allows us to study how models behave in scenarios where there is not a single, correct, ground-truth answer.
In a time where LLMs are employed simultaneously to many users with potentially different preferences, this question has become very practically relevant.
To do so, we consider how the probability distribution of the models over the three possible labels \textit{Entailment}, \textit{Neutral}, and \textit{Contradiction}) compares with the label distributions observed in the human annotations.
Following \citet{nie-etal-2020-learn}, we consider Jensen-Shannon Divergence \citep[JSD][]{menendez1997jensen} to measure the distance between the two distributions. Mathematically,

\[
    JSD(p || q) = \sqrt{\frac{1}{2}KL(p || m) + \frac{1}{2}KL(q || m)}
\]

where $p$ is the human annotator distribution, $q$ is the softmax probability distribution, and $m = \frac{1}{2}(p + q)$. Contrary to Kullback Leibner (KL) divergence, JSD is symmetric and bound between 0 and 1, making it more interpretable for our specific case.

\begin{figure*}[t]
    \begin{subfigure}[b]{\textwidth}
        %\centering
        \includegraphics[width=0.2\textwidth]{figures/chaos_jsd_training_legend}
        \vspace{-2mm}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[height=5cm]{figures/abductivenli_intermediate_jsd}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[height=5cm, trim=11mm 0 0 0, clip]{figures/mnli_matched_intermediate_jsd}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[height=5cm, trim=11mm 0 0 0, clip]{figures/snli_intermediate_jsd}
        \caption{}
    \end{subfigure}
    \caption{\textbf{Development of JSD during training.}}\label{fig:jsd_training}
\end{figure*}

In \cref{fig:jsd_per_model}, we can see that, for MNLI, all considered models have probability distributions more similar to the human distributions than chance, but substantially more different than humans have among each other.\footnote{The human estimate was computed by \citet{nie-etal-2020-learn}, on an independent sample of human annotations on the same data.}
This pattern is constant across datasets. \dieuwke{Maybe just make a row with three barplots of the three datasets?}
Interestingly, larger models appear to have lower distributions, contradicting the finding of \citet{nie-etal-2020-learn} that better or larger models do not have more similar distributions.
Yet, the effect of scale is further confirmed in \cref{fig:jsd_training}: though even trained-out models are far away from a low JSD, the JSD of the model softmax distributions and the human label distributions goes down steadily during training.
This is an exciting finding, because it suggests that measuring similarity with human distributions could be an interesting venue to explore during training.

Next, we consider the distribution of accuracy with entropy for the three benchmarks in Figure \ref{fig:entropy_accuracy}. We only show the smallest (8B) and the largest (405B) model in the Llama series across the three benchmarks. We observe that for the largest model, there's a clear decreasing trend of accuracy with increasing entropy for all three benchmarks, whereas for the 8B model, the trend is not that prominent and some entropy bins have similar accuracies. For the entropy vs accuracy distributions on other models, we refer the reader to Appendix \ref{sec:appendix}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/entropy_acc_8_405}
    \caption{Accuracy vs entropy plots for the Llama series of models for the three subsets in ChaosNLI.}
    \label{fig:entropy_accuracy}
\end{figure*}
% 
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/entropy_jsd.png}
%     \caption{JSD vs entropy plots for the Llama series of models for the three subsets in ChaosNLI.}
%     \label{fig:entropy_jsd}
% \end{figure*}
