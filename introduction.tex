\section{Introduction}

Before the state-of-the-art in NLP was constituted almost exclusively by large language models (LLMs), a popular way of evaluating models' understanding of natural language was to consider their ability to conduct \emph{natural language inference} (NLI) tasks \citep[most famously,][]{bowman-etal-2015-large,williams-etal-2018-broad}.
In this task, motivated by the idea that concepts such as entailment and contradiction are central to many aspects of language meaning \citep{bowman-etal-2015-large},  a model is asked to judge the relationship between the meaning of two sentences, typically chosing between entailment, contradiction and no relationship.
Included in then natural language understanding benchmark GLUE \citep{wang2019glue}, the NLI benchmark MNLI \citep{williams-etal-2018-broad} was up until relatively recently one of the most popular benchmarks to evaluate language models, and even in the recent past.\footnote{Over 600 times in the year of 2024 at the moment of writing.}
With the arrival of LLMs, however, MNLI and other NLI datasets have lost their popularity.
With the exception of \citet{brown2020languagemodelsfewshotlearners},  who reported very low scores for GPT-3 on the NLI benchmark ANLI \citep{nie-etal-2020-adversarial}, not a single LLM release paper considers an NLI benchmark in their evaluation suite.\footnote{\dieuwke{Though some recent papers do report poor scores ... include two recent papers.}}
In this paper, we investigate why.
Are NLI benchmarks too difficult or too easy for modern-day LLMs?
Are their scores for some reason not informative?
Or do they in fact still provide a useful signal, but has the community simply forgotten about them?

With a range of experiments, we investigate \dieuwke{...}


% We do a comprehensive analysis on five NLI benchmarks: ANLI \citep{nie-etal-2020-adversarial}, HansNLI \citep{mccoy-etal-2019-right}, MNLI \citep{williams-etal-2018-broad}, SNLI \citep{bowman-etal-2015-large}, and AbductiveNLI \citep{bhagavatula2020abductive}. The number of examples and labels for each dataset are present in Table \ref{tab:dataset}. We filter out the noisy samples from MNLI and SNLI where there's no ground truth available for the evaluation.
