\section{Introduction}

Before the state-of-the-art (SOTA) in NLP was constituted almost exclusively by large language models (LLMs), a popular way of evaluating models' understanding of natural language was to consider their ability to conduct \emph{natural language inference} (NLI) tasks \citep[most famously,][]{bowman-etal-2015-large,williams-etal-2018-broad}.
In this task, motivated by the idea that concepts such as entailment and contradiction are central to many aspects of language meaning \citep{bowman-etal-2015-large},  a model is asked to judge the relationship between the meaning of two sentences, typically chosing between entailment, contradiction and no relationship.
Included in then natural language understanding benchmark GLUE \citep{wang2019glue}, the NLI benchmark MNLI \citep{williams-etal-2018-broad} was up until relatively recently one of the most popular benchmarks to evaluate language models, and is -- with over 600 citations to date in 2024 -- well-cited even in the recent past.
However, with the arrival of LLMs, MNLI and other datasets have lost their spot on the SOTA leaderboards.
With the exception of \citet{brown2020language},  who reported very low scores for GPT-3 on the NLI benchmark ANLI \citep{nie-etal-2020-adversarial}, not a single LLM release paper considers an NLI benchmark in their evaluation suite.\footnote{Some recent papers have furthermore noted low scores for GPT3.5 for XNLI \citep{ohmer2024form,ohmer-etal-2023-separating} and for Llama 1 models on ANLI, HANS \citep{mccoy-etal-2019-right} and MNLI \citep{weber-etal-2023-mind}.}
In this paper, we investigate why.
Are NLI benchmarks too difficult or too easy for modern-day LLMs?
Are their scores for some reason not informative?
Or do they in fact still provide a useful signal, but has the community simply forgotten about them?

With a range of experiments, we investigate \dieuwke{...}


% We do a comprehensive analysis on five NLI benchmarks: ANLI \citep{nie-etal-2020-adversarial}, HansNLI \citep{mccoy-etal-2019-right}, MNLI \citep{williams-etal-2018-broad}, SNLI \citep{bowman-etal-2015-large}, and AbductiveNLI \citep{bhagavatula2020abductive}. The number of examples and labels for each dataset are present in Table \ref{tab:dataset}. We filter out the noisy samples from MNLI and SNLI where there's no ground truth available for the evaluation.
