\input{benchmarks}

\section{Entropy vs accuracy plots}\label{app:entropy_vs_accuracy}

In addition to \cref{fig:entropy_accuracy} highlighting the results on the Llama-3.1 8B and 405B models, we also show the accuracy vs entropy plots of all other models we use for our analysis: Llama-3.1 70B, Mistral 7B, Mixtral 8x7B, and Mixtral 8x22B in \cref{fig:entropy_accuracy_all}. The Mistral family of models also show a similar trend where larger models have lower accuracies for higher entropy buckets.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_abductivenli_70B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_abductivenli_7B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_abductivenli_8x7B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_abductivenli_8x22B}
        % \caption{}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_mnli_matched_70B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_mnli_matched_7B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_mnli_matched_8x7B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_mnli_matched_8x22B}
        % \caption{}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_snli_70B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_snli_7B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_snli_8x7B}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[height=3.6cm]{figures/appendix/entropy_acc_snli_8x22B}
        % \caption{}
    \end{subfigure}
    \caption{\textbf{Accuracy versus entropy.} We show how the accuracy of Llama-3.1 70B and the Mistral series of models changes as the entropy of the human label distributions increases.}
    \label{fig:entropy_accuracy_all}
\end{figure*}

\section{Prompt templates}

The prompt templates used for each task are presented in \cref{tab:prompt_template}.

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lp{8cm}}
        \toprule
        \textbf{Benchmark} & \textbf{Prompt Template} \\
        \midrule
        MNLI, SNLI, ANLI & \begin{verbatim}
{% for x in few_shot -%}
Premise: {{ x["premise"] }}
Hypothesis: {{ x["hypothesis"] }}
A. Entailment
B. Neutral
C. Contradiction
Answer: {{ x["answer"] }}

{% endfor -%}
Premise: {{ premise }}
Hypothesis: {{ hypothesis }}
A. Entailment
B. Neutral
C. Contradiction
Answer: {{ choice_text }}
\end{verbatim} \\
\midrule
AbductiveNLI & \begin{verbatim}
{% for x in few_shot -%}
Observation 1: {{ x["obs1"] }}
Observation 2: {{ x["obs2"] }}
A. {{ x["choices"]["A"] }}
B. {{ x["choices"]["B"] }}
Answer: {{ x["answer"] }}

{% endfor -%}
Observation 1: {{ obs1 }}
Observation 2: {{ obs2 }}
A. {{ choices["A"] }}
B. {{ choices["B"] }}
Answer: {{ choice_text }}
\end{verbatim} \\
\midrule
HansNLI & \begin{verbatim}
{% for x in few_shot -%}
Premise: {{ x["premise"] }}.
Hypothesis: {{ x["hypothesis"] }}.
A. Entailment
B. Non-Entailment
Answer: {{ x["answer"] }}

{% endfor -%}
Premise: {{ premise }}.
Hypothesis: {{ hypothesis }}.
A. Entailment
B. Non-Entailment
Answer: {{ choice_text }}
\end{verbatim} \\
\bottomrule
\end{tabular}
\caption{Prompt Templates for each task}
\label{tab:prompt_template}
\end{table*}

\section{Experimental Details}
\label{appx:experiments}

For the 8B and 70B models we pre-train from scratch, we use our custom pre-training datamix, a mix of data available from publicly available sources including web data, code, and reasoning datasets. For pre-training hyperparameters, we use similar settings as reported in \citet{dubey2024llama}. We use a batch size of 4M tokens and pre-train the models for 500,000 steps, resulting in a total of 2T token training. We use 512 GPUs for a single pre-training run of both models.

For running the evaluations, we use 16 GPUs for each model comprising all five NLI benchmarks and different shot settings in a single job. A single evaluation job takes on average takes around 55 mins for the 5 benchmarks.
